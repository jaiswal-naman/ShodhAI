<div align="center">

# ğŸ”¬ ShodhAI

### *Autonomous AI Research Report Generation Platform*

[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-Stateful_AI-FF6B35?style=for-the-badge)](https://langchain-ai.github.io/langgraph/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://docker.com)
[![Azure](https://img.shields.io/badge/Azure-Deployable-0078D4?style=for-the-badge&logo=microsoftazure&logoColor=white)](https://azure.microsoft.com)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

**ShodhAI** (à¤¶à¥‹à¤§ = *Research* in Hindi) is a full-stack AI platform that autonomously generates comprehensive, publication-ready research reports on any topic â€” powered by multi-agent orchestration, real-time web research, and human-in-the-loop refinement.

[Features](#-key-features) Â· [HLD](#-high-level-design-hld) Â· [LLD](#-low-level-design-lld) Â· [System Design](#-system-design) Â· [Setup](#-getting-started) Â· [Deployment](#-deployment)

</div>

---

## ğŸ¯ The Problem

Researching a topic deeply takes hours â€” reading multiple sources, cross-referencing information, synthesizing insights, and structuring everything into a coherent report. Traditional tools just assist with search; they don't actually *think*, *analyze*, or *write* for you.

## ğŸ’¡ The Solution

**ShodhAI** automates the entire research lifecycle. It doesn't just search â€” it deploys a team of AI analyst personas that each approach the topic from a different angle, conduct structured interviews backed by real-time web data, and collaboratively produce a multi-section research report with proper citations. The result is a downloadable, publication-ready document in DOCX or PDF format.

---

## âœ¨ Key Features

### ğŸ¤– Multi-Agent Research Pipeline
- Dynamically generates **diverse AI analyst personas** (technical, ethical, business, policy, etc.) tailored to each topic
- Each analyst independently conducts a structured interview with an AI expert, asking probing questions from their unique perspective
- Parallel execution ensures comprehensive coverage of the topic

### ğŸŒ Real-Time Web Research
- Integrated with **Tavily Search API** for real-time web data retrieval
- AI agents autonomously formulate search queries based on interview context
- Sources are cited and traceable throughout the final report

### ğŸ§  LangGraph-Powered Orchestration
- Built on **LangGraph** â€” a stateful, graph-based AI workflow engine
- Complex DAG (Directed Acyclic Graph) with conditional edges, parallel branches, and interrupt points
- Full state persistence with checkpointing for resumable workflows

### ğŸ‘¤ Human-in-the-Loop Refinement
- After AI generates analyst personas, the user can provide **real-time feedback** to refine perspectives
- Interrupt-resume architecture allows the pipeline to pause, accept input, and continue seamlessly
- Iterative refinement until the user is satisfied with the research direction

### ğŸ“„ Multi-Format Export
- Reports automatically exported as both **DOCX** and **PDF**
- Structured with proper headings, sections, introduction, conclusion, and source citations
- Smart text wrapping, centered layout, and page management for PDF output

### ğŸ” User Authentication System
- Secure **signup/login** with bcrypt password hashing
- Session-based authentication with cookie management
- SQLAlchemy ORM with SQLite for user data persistence

### ğŸ¨ Clean Web Interface
- Responsive **FastAPI + Jinja2** web UI with glassmorphism-inspired design
- Real-time loading spinners during report generation
- Password visibility toggle, form validation, and download buttons
- Gradient backgrounds with smooth fade-in animations

### ğŸ“Š Structured Logging & Error Handling
- **Structlog** JSON-based structured logging (console + file)
- Custom exception hierarchy with full traceback capture
- Timestamped log files for audit trails

### ğŸ³ Production-Ready Infrastructure
- Multi-stage **Dockerfile** for optimized container images
- **Jenkinsfile** CI/CD pipeline for automated testing and deployment
- **Azure Container Apps** deployment with secrets management
- Health check endpoint for container orchestration

---

## ğŸ›ï¸ High-Level Design (HLD)

The platform follows a **layered architecture** with clear separation between the Presentation, Application, AI Orchestration, and Infrastructure layers.

```mermaid
graph TB
    subgraph PL["ğŸ–¥ï¸ Presentation Layer"]
        UI["Web UI<br/>(Jinja2 Templates)"]
        CSS["Static Assets<br/>(CSS/JS)"]
    end

    subgraph AL["âš™ï¸ Application Layer"]
        API["FastAPI Server<br/>(Routes + CORS)"]
        AUTH["Auth Service<br/>(Signup/Login)"]
        RS["Report Service<br/>(Orchestrator)"]
    end

    subgraph OL["ğŸ§  AI Orchestration Layer"]
        LG["LangGraph Engine<br/>(Stateful DAG)"]
        AP["Analyst Persona<br/>Generator"]
        IW["Interview<br/>Workflow"]
        RW["Report Writer<br/>& Compiler"]
    end

    subgraph DL["ğŸ’¾ Data Layer"]
        DB["SQLite<br/>(User Auth)"]
        FS["File System<br/>(Generated Reports)"]
        CP["In-Memory<br/>Checkpointer"]
    end

    subgraph EL["ğŸŒ External Services"]
        LLM["LLM Providers<br/>(OpenAI / Gemini / Groq)"]
        TS["Tavily Search<br/>(Web Research)"]
    end

    UI --> API
    CSS --> UI
    API --> AUTH
    API --> RS
    AUTH --> DB
    RS --> LG
    LG --> AP
    LG --> IW
    LG --> RW
    AP --> LLM
    IW --> LLM
    IW --> TS
    RW --> LLM
    RW --> FS
    LG --> CP

    style PL fill:#1a1a2e,stroke:#16213e,color:#e0e0e0
    style AL fill:#16213e,stroke:#0f3460,color:#e0e0e0
    style OL fill:#0f3460,stroke:#533483,color:#e0e0e0
    style DL fill:#533483,stroke:#e94560,color:#e0e0e0
    style EL fill:#e94560,stroke:#e94560,color:#ffffff
```

### HLD â€” Component Interaction Overview

```mermaid
flowchart LR
    User([ğŸ‘¤ User]) -->|"1. Enter Topic"| Dashboard["ğŸ“Š Dashboard"]
    Dashboard -->|"POST /generate_report"| FastAPI["âš¡ FastAPI"]
    FastAPI -->|"2. Invoke"| ReportService["ğŸ”§ Report Service"]
    ReportService -->|"3. Start Pipeline"| LangGraph["ğŸ§  LangGraph<br/>Workflow Engine"]
    LangGraph -->|"4. Generate"| Analysts["ğŸ¤– AI Analysts"]
    LangGraph -.->|"5. Interrupt"| Feedback["ğŸ‘¤ Human Feedback"]
    Feedback -.->|"6. Resume"| LangGraph
    LangGraph -->|"7. Fan-out"| Interviews["ğŸ™ï¸ Parallel<br/>Interviews"]
    Interviews -->|"8. Search"| Tavily["ğŸŒ Tavily API"]
    Interviews -->|"8. Reason"| LLM["ğŸ¤– LLM Provider"]
    LangGraph -->|"9. Compile"| Report["ğŸ“„ Report<br/>Assembly"]
    Report -->|"10. Export"| Files["ğŸ“ DOCX + PDF"]
    Files -->|"11. Download"| User
```

---

## ğŸ”§ Low-Level Design (LLD)

### LLD 1 â€” Main Report Generation Graph (LangGraph DAG)

This is the core state machine that orchestrates the entire report pipeline. Each node is a function that reads/writes to a shared `ResearchGraphState`.

```mermaid
stateDiagram-v2
    [*] --> CreateAnalysts: START

    CreateAnalysts --> HumanFeedback: analysts generated

    HumanFeedback --> ConductInterview1: feedback received âœ…
    HumanFeedback --> ConductInterview2: (parallel fan-out via Send API)
    HumanFeedback --> ConductInterview3: one interview per analyst
    HumanFeedback --> [*]: no analysts / END

    state "Interview Sub-Graph" as ConductInterview1
    state "Interview Sub-Graph" as ConductInterview2
    state "Interview Sub-Graph" as ConductInterview3

    ConductInterview1 --> WriteReport: sections[]
    ConductInterview2 --> WriteIntroduction: sections[]
    ConductInterview3 --> WriteConclusion: sections[]

    WriteReport --> FinalizeReport
    WriteIntroduction --> FinalizeReport
    WriteConclusion --> FinalizeReport

    FinalizeReport --> [*]: final_report assembled

    note right of HumanFeedback
        ğŸ’¡ interrupt_before
        Pipeline pauses here for
        human analyst feedback
    end note

    note right of FinalizeReport
        ğŸ“„ Joins introduction +
        content + conclusion +
        sources into final string
    end note
```

### LLD 2 â€” Interview Sub-Graph (Per Analyst)

Each analyst runs through this independent sub-graph. The `max_num_turns` parameter controls interview depth.

```mermaid
flowchart TD
    START(("â–¶ START")) --> AQ["ğŸ¤ Ask Question<br/><i>Analyst generates question<br/>based on persona</i>"]
    AQ --> SW["ğŸ” Search Web<br/><i>LLM generates search query<br/>â†’ Tavily API retrieval</i>"]
    SW --> GA["ğŸ’¡ Generate Answer<br/><i>Expert answers using<br/>retrieved context + citations</i>"]
    GA --> SI["ğŸ’¾ Save Interview<br/><i>Serialize conversation<br/>to transcript string</i>"]
    SI --> WS["âœï¸ Write Section<br/><i>Technical writer creates<br/>structured report section</i>"]
    WS --> END_NODE(("â¹ END"))

    style START fill:#10b981,stroke:#059669,color:#fff
    style END_NODE fill:#ef4444,stroke:#dc2626,color:#fff
    style AQ fill:#3b82f6,stroke:#2563eb,color:#fff
    style SW fill:#f59e0b,stroke:#d97706,color:#fff
    style GA fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style SI fill:#06b6d4,stroke:#0891b2,color:#fff
    style WS fill:#ec4899,stroke:#db2777,color:#fff
```

### LLD 3 â€” State Models (Pydantic + TypedDict)

```mermaid
classDiagram
    class Analyst {
        +str name
        +str role
        +str affiliation
        +str description
        +persona() str
    }

    class Perspectives {
        +List~Analyst~ analysts
    }

    class SearchQuery {
        +str search_query
    }

    class GenerateAnalystsState {
        +str topic
        +int max_analysts
        +str human_analyst_feedback
        +List~Analyst~ analysts
    }

    class InterviewState {
        +int max_num_turns
        +list context
        +Analyst analyst
        +str interview
        +list sections
        +list messages
    }

    class ResearchGraphState {
        +str topic
        +int max_analysts
        +str human_analyst_feedback
        +List~Analyst~ analysts
        +list sections
        +str introduction
        +str content
        +str conclusion
        +str final_report
    }

    Perspectives --> Analyst : contains
    GenerateAnalystsState --> Analyst : references
    InterviewState --> Analyst : uses
    ResearchGraphState --> Analyst : contains
    ResearchGraphState --|> GenerateAnalystsState : extends
```

### LLD 4 â€” API Route Design

```mermaid
sequenceDiagram
    actor User
    participant UI as Web UI
    participant API as FastAPI Router
    participant Auth as Auth Service
    participant DB as SQLite DB
    participant RS as Report Service
    participant LG as LangGraph
    participant LLM as LLM Provider
    participant TV as Tavily Search

    User->>UI: GET / (Login Page)
    UI-->>User: login.html

    User->>API: POST /login (username, password)
    API->>Auth: verify_password()
    Auth->>DB: query User
    DB-->>Auth: user record
    Auth-->>API: session_id cookie
    API-->>User: 302 â†’ /dashboard

    User->>API: POST /generate_report (topic)
    API->>RS: start_report_generation(topic, 3)
    RS->>LG: graph.stream(topic, max_analysts)
    LG->>LLM: create analyst personas
    LLM-->>LG: List[Analyst]
    LG-->>RS: thread_id (paused at human_feedback)
    RS-->>API: thread_id
    API-->>User: report_progress.html

    User->>API: POST /submit_feedback (feedback, thread_id)
    API->>RS: submit_feedback(thread_id, feedback)
    RS->>LG: update_state â†’ resume pipeline

    loop For Each Analyst (Parallel)
        LG->>LLM: generate interview question
        LG->>TV: web search
        TV-->>LG: search results
        LG->>LLM: generate expert answer
        LG->>LLM: write report section
    end

    LG->>LLM: write introduction + conclusion (parallel)
    LG->>LG: finalize_report()
    RS->>RS: save_report(DOCX + PDF)
    RS-->>API: doc_path, pdf_path
    API-->>User: download links

    User->>API: GET /download/report.pdf
    API-->>User: ğŸ“„ FileResponse
```

---

## ğŸ—ï¸ System Design

### System Context Diagram

```mermaid
graph TB
    User([ğŸ‘¤ Researcher / User])

    subgraph ShodhAI["ğŸ”¬ ShodhAI Platform"]
        WebApp["Web Application<br/>(FastAPI + Jinja2)"]
        AIEngine["AI Research Engine<br/>(LangGraph + LLMs)"]
        ExportEngine["Export Engine<br/>(DOCX + PDF)"]
        AuthSystem["Auth System<br/>(SQLAlchemy + bcrypt)"]
    end

    OpenAI["â˜ï¸ OpenAI API<br/>(GPT-4o)"]
    Google["â˜ï¸ Google API<br/>(Gemini 2.0 Flash)"]
    Groq["â˜ï¸ Groq API<br/>(DeepSeek R1)"]
    Tavily["â˜ï¸ Tavily API<br/>(Web Search)"]

    User <-->|"HTTP / Browser"| WebApp
    WebApp --> AIEngine
    WebApp --> AuthSystem
    AIEngine --> ExportEngine
    AIEngine <-->|"LLM Inference"| OpenAI
    AIEngine <-->|"LLM Inference"| Google
    AIEngine <-->|"LLM Inference"| Groq
    AIEngine <-->|"Web Search"| Tavily

    style ShodhAI fill:#0f172a,stroke:#334155,color:#e2e8f0
    style User fill:#3b82f6,stroke:#2563eb,color:#fff
    style OpenAI fill:#10a37f,stroke:#10a37f,color:#fff
    style Google fill:#4285f4,stroke:#4285f4,color:#fff
    style Groq fill:#f55036,stroke:#f55036,color:#fff
    style Tavily fill:#ff6b35,stroke:#ff6b35,color:#fff
```

### Request Flow â€” Complete Data Pipeline

```mermaid
flowchart TD
    A["ğŸ‘¤ User submits topic<br/>'Impact of AI on Healthcare'"] --> B["âš¡ FastAPI receives<br/>POST /generate_report"]
    B --> C["ğŸ”§ ReportService<br/>creates thread_id"]
    C --> D{"ğŸ§  LangGraph<br/>Pipeline Start"}

    D --> E["ğŸ¤– CreateAnalysts Node<br/>LLM generates N personas"]
    E --> F["â¸ï¸ HumanFeedback Node<br/>(interrupt_before)"]

    F -->|"User provides feedback"| G{"Feedback<br/>Empty?"}
    G -->|"No â€” refine"| E
    G -->|"Yes â€” proceed"| H["ğŸ“¡ Fan-Out via Send() API"]

    H --> I1["ğŸ™ï¸ Analyst #1<br/>Interview Sub-Graph"]
    H --> I2["ğŸ™ï¸ Analyst #2<br/>Interview Sub-Graph"]
    H --> I3["ğŸ™ï¸ Analyst #3<br/>Interview Sub-Graph"]

    I1 --> J["ğŸ“ Sections Collected<br/>(Annotated list with operator.add)"]
    I2 --> J
    I3 --> J

    J --> K1["âœï¸ Write Report<br/>(consolidate sections)"]
    J --> K2["âœï¸ Write Introduction"]
    J --> K3["âœï¸ Write Conclusion"]

    K1 --> L["ğŸ”— Finalize Report<br/>intro + content + conclusion + sources"]
    K2 --> L
    K3 --> L

    L --> M["ğŸ’¾ Save Report<br/>DOCX + PDF export"]
    M --> N["ğŸ“¥ User Downloads<br/>GET /download/filename"]

    style A fill:#3b82f6,stroke:#2563eb,color:#fff
    style F fill:#f59e0b,stroke:#d97706,color:#fff
    style H fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style L fill:#10b981,stroke:#059669,color:#fff
    style N fill:#ec4899,stroke:#db2777,color:#fff
```

### CI/CD Pipeline Architecture

```mermaid
flowchart LR
    subgraph DEV["ğŸ‘¨â€ğŸ’» Development"]
        Code["Source Code<br/>(GitHub)"]
    end

    subgraph CI["ğŸ”„ Continuous Integration"]
        Checkout["ğŸ“¥ Checkout"]
        Setup["ğŸ Python Setup"]
        Install["ğŸ“¦ Install Deps"]
        Test["âœ… Run Tests"]
    end

    subgraph CD["ğŸš€ Continuous Deployment"]
        Build["ğŸ³ Docker Build<br/>(Multi-stage)"]
        Push["ğŸ“¤ Push to ACR<br/>(Azure Container Registry)"]
        Deploy["â˜ï¸ Deploy to<br/>Azure Container Apps"]
        Verify["âœ”ï¸ Health Check<br/>/health endpoint"]
    end

    subgraph PROD["ğŸŒ Production"]
        App["ğŸ”¬ ShodhAI App<br/>(Container Instance)"]
        Secrets["ğŸ” Azure Secrets<br/>(API Keys)"]
    end

    Code --> Checkout --> Setup --> Install --> Test
    Test --> Build --> Push --> Deploy --> Verify
    Deploy --> App
    Secrets --> App

    style DEV fill:#1e293b,stroke:#334155,color:#e2e8f0
    style CI fill:#1e3a5f,stroke:#2563eb,color:#e2e8f0
    style CD fill:#14532d,stroke:#16a34a,color:#e2e8f0
    style PROD fill:#7c2d12,stroke:#ea580c,color:#e2e8f0
```

### Deployment Architecture

```mermaid
graph TB
    subgraph AZURE["â˜ï¸ Azure Cloud"]
        subgraph RG["Resource Group: shodhai-app-rg"]
            subgraph ACR["Azure Container Registry"]
                IMG["shodhai-app:latest"]
            end

            subgraph ENV["Container Apps Environment"]
                APP["ğŸ”¬ ShodhAI Container<br/>Port 8000<br/>1 CPU Â· 2GB RAM<br/>Min: 1 Â· Max: 3 replicas"]
            end

            subgraph SECRETS["Container Secrets"]
                S1["OPENAI_API_KEY"]
                S2["GOOGLE_API_KEY"]
                S3["GROQ_API_KEY"]
                S4["TAVILY_API_KEY"]
            end
        end

        subgraph JENKINS_RG["Resource Group: shodhai-jenkins-rg"]
            JENKINS["ğŸ”§ Jenkins Container<br/>Port 8080<br/>2 CPU Â· 4GB RAM"]
            STORAGE["ğŸ“ Azure File Share<br/>(Jenkins persistent data)"]
        end
    end

    INTERNET(("ğŸŒ Internet")) <-->|"HTTPS"| APP
    JENKINS -->|"Build & Deploy"| ACR
    ACR -->|"Pull Image"| APP
    SECRETS -->|"Inject"| APP
    STORAGE -->|"Mount"| JENKINS

    style AZURE fill:#0f172a,stroke:#1e40af,color:#e2e8f0
    style RG fill:#1e293b,stroke:#334155,color:#e2e8f0
    style JENKINS_RG fill:#1e293b,stroke:#334155,color:#e2e8f0
    style APP fill:#059669,stroke:#10b981,color:#fff
    style JENKINS fill:#2563eb,stroke:#3b82f6,color:#fff
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **AI Orchestration** | LangGraph | Stateful multi-agent workflow with checkpointing |
| **LLM Providers** | OpenAI GPT-4o / Google Gemini / Groq | Flexible multi-provider LLM support |
| **Web Search** | Tavily API | Real-time web research with source attribution |
| **Backend** | FastAPI + Uvicorn | High-performance async API server |
| **Frontend** | Jinja2 + Vanilla CSS + JS | Server-rendered responsive web UI |
| **Database** | SQLAlchemy + SQLite | User authentication & session management |
| **Security** | bcrypt + Passlib | Password hashing & verification |
| **Document Export** | python-docx + ReportLab | DOCX and PDF report generation |
| **Logging** | Structlog | JSON-structured logging with file persistence |
| **Containerization** | Docker (multi-stage) | Optimized production container images |
| **CI/CD** | Jenkins | Automated build, test, and deployment pipeline |
| **Cloud** | Azure Container Apps | Scalable serverless container deployment |

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.11+
- API keys for at least one LLM provider
- Tavily API key for web search

### Installation

```bash
# Clone the repository
git clone https://github.com/jaiswal-naman/ShodhAI.git
cd ShodhAI

# Create and activate virtual environment
python -m venv venv
.\venv\Scripts\activate        # Windows
# source venv/bin/activate     # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### Configuration

```bash
# Copy the environment template
cp .env.copy .env
```

Edit `.env` with your API keys:

```env
GROQ_API_KEY=your_groq_key_here
GOOGLE_API_KEY=your_google_key_here
OPENAI_API_KEY=your_openai_key_here
TAVILY_API_KEY=your_tavily_key_here
LLM_PROVIDER=openai    # Options: openai, google, groq
```

### Run

```bash
uvicorn research_and_analyst.api.main:app --reload
```

Visit **http://localhost:8000** â†’ Sign up â†’ Enter a topic â†’ Get your AI-generated report!

---

## ğŸ³ Deployment

### Docker

```bash
docker build -t shodhai .
docker run -p 8000:8000 --env-file .env shodhai
```

### Azure Container Apps

```bash
# 1. Setup infrastructure
./setup-app-infrastructure.sh

# 2. Build and push Docker image
./build-and-push-docker-image.sh

# 3. Deploy via Jenkins pipeline (or manually)
```

---

## ğŸ“ Project Structure

```
ShodhAI/
â”œâ”€â”€ research_and_analyst/              # Core application package
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI app initialization & CORS
â”‚   â”‚   â”œâ”€â”€ routes/report_routes.py    # Auth + report generation endpoints
â”‚   â”‚   â”œâ”€â”€ services/report_service.py # Business logic & workflow orchestration
â”‚   â”‚   â””â”€â”€ templates/                 # Jinja2 HTML templates (4 pages)
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ report_generator_workflow.py  # Main LangGraph DAG (7 nodes)
â”‚   â”‚   â””â”€â”€ interview_workflow.py         # Interview sub-graph (5 nodes)
â”‚   â”œâ”€â”€ schemas/models.py              # Pydantic models & TypedDict states
â”‚   â”œâ”€â”€ config/configuration.yaml      # Multi-provider LLM configuration
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ model_loader.py            # Dynamic LLM/embedding factory
â”‚   â”‚   â””â”€â”€ config_loader.py           # YAML config with env override
â”‚   â”œâ”€â”€ prompt_lib/prompt_locator.py   # 6 Jinja2 prompt templates
â”‚   â”œâ”€â”€ database/db_config.py          # SQLAlchemy models & auth helpers
â”‚   â”œâ”€â”€ logger/                        # Structlog JSON logger
â”‚   â””â”€â”€ exception/                     # Custom exception with traceback
â”œâ”€â”€ static/css/styles.css              # UI styling
â”œâ”€â”€ Dockerfile                         # Multi-stage production build
â”œâ”€â”€ Dockerfile.jenkins                 # Jenkins CI server image
â”œâ”€â”€ Jenkinsfile                        # Full CI/CD pipeline
â”œâ”€â”€ azure-deploy-jenkins.sh            # Jenkins Azure deployment
â”œâ”€â”€ setup-app-infrastructure.sh        # Azure infra provisioning
â””â”€â”€ build-and-push-docker-image.sh     # Docker build & ACR push
```

---

## ğŸ”® Future Roadmap

- [ ] RAG integration for document-based research (PDF/URL upload)
- [ ] Streaming response for real-time report generation progress
- [ ] Multi-language report generation
- [ ] Research history dashboard with saved reports
- [ ] Collaborative research sessions with multiple users
- [ ] Advanced analytics on research quality and source diversity

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ by [Naman Jaiswal](https://github.com/jaiswal-naman)**

*ShodhAI â€” Because research should be intelligent, autonomous, and effortless.*

</div>